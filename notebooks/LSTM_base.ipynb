{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a996c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888da469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Export</th>\n",
       "      <th>Import</th>\n",
       "      <th>Year</th>\n",
       "      <th>Export_Value</th>\n",
       "      <th>Export_Value_(t-1)</th>\n",
       "      <th>GDP_i(t-1)</th>\n",
       "      <th>GDP_j(t-1)</th>\n",
       "      <th>GDPPC_i(t-1)</th>\n",
       "      <th>GDPPC_j(t-1)</th>\n",
       "      <th>D_ij</th>\n",
       "      <th>FTA_1</th>\n",
       "      <th>FTA_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>China</td>\n",
       "      <td>2000</td>\n",
       "      <td>11191.652</td>\n",
       "      <td>5022.337</td>\n",
       "      <td>8.982048e+09</td>\n",
       "      <td>1.093997e+12</td>\n",
       "      <td>32381.625236</td>\n",
       "      <td>873.287062</td>\n",
       "      <td>7861.07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>France</td>\n",
       "      <td>2000</td>\n",
       "      <td>87202.089</td>\n",
       "      <td>104187.632</td>\n",
       "      <td>8.982048e+09</td>\n",
       "      <td>1.492648e+12</td>\n",
       "      <td>32381.625236</td>\n",
       "      <td>24673.203048</td>\n",
       "      <td>2372.32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2000</td>\n",
       "      <td>312116.389</td>\n",
       "      <td>262963.856</td>\n",
       "      <td>8.982048e+09</td>\n",
       "      <td>2.194204e+12</td>\n",
       "      <td>32381.625236</td>\n",
       "      <td>26725.915218</td>\n",
       "      <td>2246.80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2000</td>\n",
       "      <td>27966.976</td>\n",
       "      <td>34470.260</td>\n",
       "      <td>8.982048e+09</td>\n",
       "      <td>1.252024e+12</td>\n",
       "      <td>32381.625236</td>\n",
       "      <td>21997.624316</td>\n",
       "      <td>3138.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>2000</td>\n",
       "      <td>147015.380</td>\n",
       "      <td>120782.848</td>\n",
       "      <td>8.982048e+09</td>\n",
       "      <td>4.468986e+11</td>\n",
       "      <td>32381.625236</td>\n",
       "      <td>28263.096711</td>\n",
       "      <td>1932.09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Export       Import  Year  Export_Value  Export_Value_(t-1)    GDP_i(t-1)  \\\n",
       "0  Iceland        China  2000     11191.652            5022.337  8.982048e+09   \n",
       "1  Iceland       France  2000     87202.089          104187.632  8.982048e+09   \n",
       "2  Iceland      Germany  2000    312116.389          262963.856  8.982048e+09   \n",
       "3  Iceland        Italy  2000     27966.976           34470.260  8.982048e+09   \n",
       "4  Iceland  Netherlands  2000    147015.380          120782.848  8.982048e+09   \n",
       "\n",
       "     GDP_j(t-1)  GDPPC_i(t-1)  GDPPC_j(t-1)     D_ij  FTA_1  FTA_2  \n",
       "0  1.093997e+12  32381.625236    873.287062  7861.07      0      1  \n",
       "1  1.492648e+12  32381.625236  24673.203048  2372.32      0      1  \n",
       "2  2.194204e+12  32381.625236  26725.915218  2246.80      0      1  \n",
       "3  1.252024e+12  32381.625236  21997.624316  3138.62      0      1  \n",
       "4  4.468986e+11  32381.625236  28263.096711  1932.09      0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv(\"data/export_data_all.csv\")\n",
    "df = dff.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998a5e4",
   "metadata": {},
   "source": [
    "From the year column is substracted 2000 to work with smaller values. Additionally, \"Export\" and \"Import\" columns are dropped, since they will not used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0402fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"] = df[\"Year\"]-2000\n",
    "df.drop([\"Export\",\"Import\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69f11c",
   "metadata": {},
   "source": [
    "Since the variables has different ranges of values, scaling is needed to get an efficient result. Due to dealing with economic data, natural logarithm is taken of the columns in the 'columns' list to normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5bbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Export_Value', 'Export_Value_(t-1)', 'GDP_i(t-1)', 'GDP_j(t-1)','GDPPC_i(t-1)', 'GDPPC_j(t-1)', 'D_ij']\n",
    "df[columns] = df[columns].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60371c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Export_Value</th>\n",
       "      <th>Export_Value_(t-1)</th>\n",
       "      <th>GDP_i(t-1)</th>\n",
       "      <th>GDP_j(t-1)</th>\n",
       "      <th>GDPPC_i(t-1)</th>\n",
       "      <th>GDPPC_j(t-1)</th>\n",
       "      <th>D_ij</th>\n",
       "      <th>FTA_1</th>\n",
       "      <th>FTA_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.322923</td>\n",
       "      <td>8.521651</td>\n",
       "      <td>22.918494</td>\n",
       "      <td>27.720859</td>\n",
       "      <td>10.385346</td>\n",
       "      <td>6.772264</td>\n",
       "      <td>8.969678</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.375984</td>\n",
       "      <td>11.553949</td>\n",
       "      <td>22.918494</td>\n",
       "      <td>28.031573</td>\n",
       "      <td>10.385346</td>\n",
       "      <td>10.113473</td>\n",
       "      <td>7.771624</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.651131</td>\n",
       "      <td>12.479772</td>\n",
       "      <td>22.918494</td>\n",
       "      <td>28.416841</td>\n",
       "      <td>10.385346</td>\n",
       "      <td>10.193389</td>\n",
       "      <td>7.717262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10.238780</td>\n",
       "      <td>10.447852</td>\n",
       "      <td>22.918494</td>\n",
       "      <td>27.855782</td>\n",
       "      <td>10.385346</td>\n",
       "      <td>9.998690</td>\n",
       "      <td>8.051538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11.898292</td>\n",
       "      <td>11.701750</td>\n",
       "      <td>22.918494</td>\n",
       "      <td>26.825597</td>\n",
       "      <td>10.385346</td>\n",
       "      <td>10.249312</td>\n",
       "      <td>7.566358</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Export_Value  Export_Value_(t-1)  GDP_i(t-1)  GDP_j(t-1)  \\\n",
       "0     0      9.322923            8.521651   22.918494   27.720859   \n",
       "1     0     11.375984           11.553949   22.918494   28.031573   \n",
       "2     0     12.651131           12.479772   22.918494   28.416841   \n",
       "3     0     10.238780           10.447852   22.918494   27.855782   \n",
       "4     0     11.898292           11.701750   22.918494   26.825597   \n",
       "\n",
       "   GDPPC_i(t-1)  GDPPC_j(t-1)      D_ij  FTA_1  FTA_2  \n",
       "0     10.385346      6.772264  8.969678      0      1  \n",
       "1     10.385346     10.113473  7.771624      0      1  \n",
       "2     10.385346     10.193389  7.717262      0      1  \n",
       "3     10.385346      9.998690  8.051538      0      1  \n",
       "4     10.385346     10.249312  7.566358      0      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fef8df",
   "metadata": {},
   "source": [
    "After taken natural logarithm of the columns, min-max normalization is appilied to obtain smalled numbers. Fit_transform() is a combination of fit and transform method. Fit method computes the mean and standard deviation of the feature and transform method applies these calculations to every data point for each feature. Since only target value, Export_Value, is needed to calculate the error after the model prediction, MinMaxScaler() is applied to target value and features separately for normalization which is scaling each data point with Equation **???**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f040532",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler_y = preprocessing.MinMaxScaler()\n",
    "df['Export_Value']= min_max_scaler_y.fit_transform(pd.DataFrame(df['Export_Value']))\n",
    "                                                           #İki boyutlu istediği için data frame çevirip atadık.     \n",
    "columns_mms=['Export_Value_(t-1)', 'GDP_i(t-1)', 'GDP_j(t-1)','GDPPC_i(t-1)', 'GDPPC_j(t-1)', 'D_ij']\n",
    "min_max_scaler_x = preprocessing.MinMaxScaler()\n",
    "df[columns_mms] = min_max_scaler_x.fit_transform(df[columns_mms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534c3342-7ec5-4a14-8d15-1761d97f561a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Export_Value</th>\n",
       "      <th>Export_Value_(t-1)</th>\n",
       "      <th>GDP_i(t-1)</th>\n",
       "      <th>GDP_j(t-1)</th>\n",
       "      <th>GDPPC_i(t-1)</th>\n",
       "      <th>GDPPC_j(t-1)</th>\n",
       "      <th>D_ij</th>\n",
       "      <th>FTA_1</th>\n",
       "      <th>FTA_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.239215</td>\n",
       "      <td>0.166455</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.624801</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915914</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.426011</td>\n",
       "      <td>0.442586</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.664507</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.522239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.542030</td>\n",
       "      <td>0.526894</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.713741</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.717310</td>\n",
       "      <td>0.504376</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>0.341861</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.642043</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.676487</td>\n",
       "      <td>0.614218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.473533</td>\n",
       "      <td>0.456045</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.510394</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.729036</td>\n",
       "      <td>0.454790</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Export_Value  Export_Value_(t-1)  GDP_i(t-1)  GDP_j(t-1)  \\\n",
       "0     0      0.239215            0.166455    0.018261    0.624801   \n",
       "1     0      0.426011            0.442586    0.018261    0.664507   \n",
       "2     0      0.542030            0.526894    0.018261    0.713741   \n",
       "3     0      0.322544            0.341861    0.018261    0.642043   \n",
       "4     0      0.473533            0.456045    0.018261    0.510394   \n",
       "\n",
       "   GDPPC_i(t-1)  GDPPC_j(t-1)      D_ij  FTA_1  FTA_2  \n",
       "0      0.668563      0.000000  0.915914      0      1  \n",
       "1      0.668563      0.700554  0.522239      0      1  \n",
       "2      0.668563      0.717310  0.504376      0      1  \n",
       "3      0.668563      0.676487  0.614218      0      1  \n",
       "4      0.668563      0.729036  0.454790      0      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbff39b",
   "metadata": {},
   "source": [
    "The dataset is divided into target value and features for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9190a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = df['Export_Value'].values\n",
    "x_values = df.drop('Export_Value',axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dead96",
   "metadata": {},
   "source": [
    "The dataset is splited into training, validation, and testing. As the dataset is a time series, the order of the dataset is important. The training, validation, and test dataseta has rows from 2000 to 2016, from 2017 to 2018, and 2019, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b2458e-7797-48dd-9cf5-d4491885e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_row = 748 # 2000-2016\n",
    "val = 836 # 2017-2018\n",
    "\n",
    "train_X = x_values[:train_row, :]\n",
    "validation_X = x_values[train_row:val, :]\n",
    "test_X = x_values[val:, :]\n",
    "\n",
    "train_y = y_values[:train_row]\n",
    "validation_y = y_values[train_row:val]\n",
    "test_y = y_values[val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d1b70",
   "metadata": {},
   "source": [
    "LSTM takes input layer in three dimension as samples, time steps, and features. So, x and y values are reshaped from 2-dimension to 3-dimension. Time step represents how many rows the model will go back while learning. If time step takes the value 1, it implies that it will look at only current row. For instance, if the time step is 3, the model will look from current row to two previous row to learn.  \n",
    "\n",
    "Time steps is taken value 1 because higher error value is received when changing the timestep with higher value than 1. Since country pairs repeat every 44 rows, especially the value of 44 has been considered for timestep value. However, as stated before, value 44 did not give as good results as value 1. When tested with value 1, obtained error values were between 0.4 and 0.6, but while tested with 44, obtained error values were bigger than 1. Moreover, since the previous year's data is taken as features while preparing the data, LSTM already estimates the export value based on the previous year's data when it only looks at the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53305c31-d953-4421-ac6b-d03369d3897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 1, 9) (748, 1) (88, 1, 9) (88, 1) (44, 1, 9) (44, 1)\n"
     ]
    }
   ],
   "source": [
    "timesteps = 1\n",
    "\n",
    "train_X = train_X.reshape((train_X.shape[0]//timesteps, timesteps, train_X.shape[1]))\n",
    "validation_X = validation_X.reshape((validation_X.shape[0]//timesteps, timesteps, validation_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0]//timesteps, timesteps, test_X.shape[1]))\n",
    "\n",
    "train_y = train_y.reshape((train_y.shape[0]//timesteps, timesteps,))\n",
    "validation_y = validation_y.reshape((validation_y.shape[0]//timesteps, timesteps,))\n",
    "test_y =  test_y.reshape((test_y.shape[0]//timesteps, timesteps,))\n",
    "\n",
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5606ba",
   "metadata": {},
   "source": [
    "There is a function called Sequential() to build models layer by layer in Keras. Each layer is followed by the next layer with its corresponding weights. The add function is used for adding layers to the model.[kaynakk](https://medium.com/ai-techsystems/mushroom-classification-using-deep-learning-e0154afa4c03)  \n",
    "\n",
    "On the first hidden layer, LSTMs input layer should be defined with input_shape argument. Two values are required for the input shape argument: the number of time steps and the number of features. LSTM presumes that sample number is 1 or more.[kaynakk](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/)  \n",
    "\n",
    "Dense class is applied for connecting layers. In the first argument of the Dense class, the number of neurons in the output layer is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7facd5a1-52a0-4612-b02a-792a9169a28d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316ff57",
   "metadata": {},
   "source": [
    "The loss function is needed to be specified for evaluating weights according to it. The optimizer adam whis is some sort of Stochastic Gradient Descent (SGD) is used for computation of the gradient and it will update the weights on small batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccec7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3feb0",
   "metadata": {},
   "source": [
    "While training the model, the number of epochs to apply is a crucial issue. If the number of epochs is high, the developed model may be over-fitting for the training dataset, while using less may result in underfitting. This problem can be avoided by applying the early stopping method. On model.fit, argument “epochs” mostly given as large numbers. The early stopping stops training of the model when it realizes the model has already learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e46bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe9862",
   "metadata": {},
   "source": [
    "The model has been specified and compiled; therefore, it is now ready for efficient computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9404a6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24/24 - 3s - loss: 0.1911 - val_loss: 0.0534 - 3s/epoch - 114ms/step\n",
      "Epoch 2/200\n",
      "24/24 - 0s - loss: 0.0788 - val_loss: 0.0516 - 71ms/epoch - 3ms/step\n",
      "Epoch 3/200\n",
      "24/24 - 0s - loss: 0.0566 - val_loss: 0.0575 - 75ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "24/24 - 0s - loss: 0.0479 - val_loss: 0.0483 - 74ms/epoch - 3ms/step\n",
      "Epoch 5/200\n",
      "24/24 - 0s - loss: 0.0421 - val_loss: 0.0411 - 83ms/epoch - 3ms/step\n",
      "Epoch 6/200\n",
      "24/24 - 0s - loss: 0.0372 - val_loss: 0.0363 - 95ms/epoch - 4ms/step\n",
      "Epoch 7/200\n",
      "24/24 - 0s - loss: 0.0329 - val_loss: 0.0324 - 83ms/epoch - 3ms/step\n",
      "Epoch 8/200\n",
      "24/24 - 0s - loss: 0.0295 - val_loss: 0.0292 - 86ms/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "24/24 - 0s - loss: 0.0267 - val_loss: 0.0268 - 84ms/epoch - 3ms/step\n",
      "Epoch 10/200\n",
      "24/24 - 0s - loss: 0.0245 - val_loss: 0.0248 - 89ms/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "24/24 - 0s - loss: 0.0227 - val_loss: 0.0230 - 91ms/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "24/24 - 0s - loss: 0.0213 - val_loss: 0.0215 - 80ms/epoch - 3ms/step\n",
      "Epoch 13/200\n",
      "24/24 - 0s - loss: 0.0200 - val_loss: 0.0202 - 78ms/epoch - 3ms/step\n",
      "Epoch 14/200\n",
      "24/24 - 0s - loss: 0.0188 - val_loss: 0.0189 - 80ms/epoch - 3ms/step\n",
      "Epoch 15/200\n",
      "24/24 - 0s - loss: 0.0177 - val_loss: 0.0178 - 76ms/epoch - 3ms/step\n",
      "Epoch 16/200\n",
      "24/24 - 0s - loss: 0.0167 - val_loss: 0.0167 - 74ms/epoch - 3ms/step\n",
      "Epoch 17/200\n",
      "24/24 - 0s - loss: 0.0157 - val_loss: 0.0156 - 68ms/epoch - 3ms/step\n",
      "Epoch 18/200\n",
      "24/24 - 0s - loss: 0.0148 - val_loss: 0.0147 - 70ms/epoch - 3ms/step\n",
      "Epoch 19/200\n",
      "24/24 - 0s - loss: 0.0138 - val_loss: 0.0137 - 76ms/epoch - 3ms/step\n",
      "Epoch 20/200\n",
      "24/24 - 0s - loss: 0.0129 - val_loss: 0.0128 - 79ms/epoch - 3ms/step\n",
      "Epoch 21/200\n",
      "24/24 - 0s - loss: 0.0119 - val_loss: 0.0119 - 76ms/epoch - 3ms/step\n",
      "Epoch 22/200\n",
      "24/24 - 0s - loss: 0.0111 - val_loss: 0.0110 - 76ms/epoch - 3ms/step\n",
      "Epoch 23/200\n",
      "24/24 - 0s - loss: 0.0102 - val_loss: 0.0102 - 81ms/epoch - 3ms/step\n",
      "Epoch 24/200\n",
      "24/24 - 0s - loss: 0.0094 - val_loss: 0.0094 - 71ms/epoch - 3ms/step\n",
      "Epoch 25/200\n",
      "24/24 - 0s - loss: 0.0086 - val_loss: 0.0087 - 80ms/epoch - 3ms/step\n",
      "Epoch 26/200\n",
      "24/24 - 0s - loss: 0.0079 - val_loss: 0.0081 - 74ms/epoch - 3ms/step\n",
      "Epoch 27/200\n",
      "24/24 - 0s - loss: 0.0072 - val_loss: 0.0075 - 77ms/epoch - 3ms/step\n",
      "Epoch 28/200\n",
      "24/24 - 0s - loss: 0.0066 - val_loss: 0.0069 - 71ms/epoch - 3ms/step\n",
      "Epoch 29/200\n",
      "24/24 - 0s - loss: 0.0060 - val_loss: 0.0065 - 71ms/epoch - 3ms/step\n",
      "Epoch 30/200\n",
      "24/24 - 0s - loss: 0.0055 - val_loss: 0.0060 - 69ms/epoch - 3ms/step\n",
      "Epoch 31/200\n",
      "24/24 - 0s - loss: 0.0051 - val_loss: 0.0057 - 73ms/epoch - 3ms/step\n",
      "Epoch 32/200\n",
      "24/24 - 0s - loss: 0.0047 - val_loss: 0.0054 - 78ms/epoch - 3ms/step\n",
      "Epoch 33/200\n",
      "24/24 - 0s - loss: 0.0044 - val_loss: 0.0051 - 77ms/epoch - 3ms/step\n",
      "Epoch 34/200\n",
      "24/24 - 0s - loss: 0.0041 - val_loss: 0.0049 - 74ms/epoch - 3ms/step\n",
      "Epoch 35/200\n",
      "24/24 - 0s - loss: 0.0038 - val_loss: 0.0047 - 69ms/epoch - 3ms/step\n",
      "Epoch 36/200\n",
      "24/24 - 0s - loss: 0.0036 - val_loss: 0.0045 - 74ms/epoch - 3ms/step\n",
      "Epoch 37/200\n",
      "24/24 - 0s - loss: 0.0034 - val_loss: 0.0043 - 72ms/epoch - 3ms/step\n",
      "Epoch 38/200\n",
      "24/24 - 0s - loss: 0.0032 - val_loss: 0.0042 - 75ms/epoch - 3ms/step\n",
      "Epoch 39/200\n",
      "24/24 - 0s - loss: 0.0031 - val_loss: 0.0041 - 73ms/epoch - 3ms/step\n",
      "Epoch 40/200\n",
      "24/24 - 0s - loss: 0.0029 - val_loss: 0.0039 - 81ms/epoch - 3ms/step\n",
      "Epoch 41/200\n",
      "24/24 - 0s - loss: 0.0028 - val_loss: 0.0038 - 77ms/epoch - 3ms/step\n",
      "Epoch 42/200\n",
      "24/24 - 0s - loss: 0.0027 - val_loss: 0.0037 - 71ms/epoch - 3ms/step\n",
      "Epoch 43/200\n",
      "24/24 - 0s - loss: 0.0026 - val_loss: 0.0036 - 74ms/epoch - 3ms/step\n",
      "Epoch 44/200\n",
      "24/24 - 0s - loss: 0.0025 - val_loss: 0.0035 - 81ms/epoch - 3ms/step\n",
      "Epoch 45/200\n",
      "24/24 - 0s - loss: 0.0024 - val_loss: 0.0034 - 77ms/epoch - 3ms/step\n",
      "Epoch 46/200\n",
      "24/24 - 0s - loss: 0.0023 - val_loss: 0.0033 - 76ms/epoch - 3ms/step\n",
      "Epoch 47/200\n",
      "24/24 - 0s - loss: 0.0023 - val_loss: 0.0032 - 82ms/epoch - 3ms/step\n",
      "Epoch 48/200\n",
      "24/24 - 0s - loss: 0.0022 - val_loss: 0.0031 - 74ms/epoch - 3ms/step\n",
      "Epoch 49/200\n",
      "24/24 - 0s - loss: 0.0021 - val_loss: 0.0031 - 78ms/epoch - 3ms/step\n",
      "Epoch 50/200\n",
      "24/24 - 0s - loss: 0.0021 - val_loss: 0.0030 - 74ms/epoch - 3ms/step\n",
      "Epoch 51/200\n",
      "24/24 - 0s - loss: 0.0020 - val_loss: 0.0029 - 78ms/epoch - 3ms/step\n",
      "Epoch 52/200\n",
      "24/24 - 0s - loss: 0.0020 - val_loss: 0.0029 - 81ms/epoch - 3ms/step\n",
      "Epoch 53/200\n",
      "24/24 - 0s - loss: 0.0019 - val_loss: 0.0028 - 86ms/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "24/24 - 0s - loss: 0.0019 - val_loss: 0.0028 - 76ms/epoch - 3ms/step\n",
      "Epoch 55/200\n",
      "24/24 - 0s - loss: 0.0018 - val_loss: 0.0027 - 85ms/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "24/24 - 0s - loss: 0.0018 - val_loss: 0.0027 - 78ms/epoch - 3ms/step\n",
      "Epoch 57/200\n",
      "24/24 - 0s - loss: 0.0017 - val_loss: 0.0026 - 82ms/epoch - 3ms/step\n",
      "Epoch 58/200\n",
      "24/24 - 0s - loss: 0.0017 - val_loss: 0.0026 - 85ms/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "24/24 - 0s - loss: 0.0017 - val_loss: 0.0025 - 73ms/epoch - 3ms/step\n",
      "Epoch 60/200\n",
      "24/24 - 0s - loss: 0.0016 - val_loss: 0.0025 - 76ms/epoch - 3ms/step\n",
      "Epoch 61/200\n",
      "24/24 - 0s - loss: 0.0016 - val_loss: 0.0025 - 80ms/epoch - 3ms/step\n",
      "Epoch 62/200\n",
      "24/24 - 0s - loss: 0.0016 - val_loss: 0.0024 - 76ms/epoch - 3ms/step\n",
      "Epoch 63/200\n",
      "24/24 - 0s - loss: 0.0015 - val_loss: 0.0024 - 85ms/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "24/24 - 0s - loss: 0.0015 - val_loss: 0.0024 - 89ms/epoch - 4ms/step\n",
      "Epoch 65/200\n",
      "24/24 - 0s - loss: 0.0015 - val_loss: 0.0023 - 74ms/epoch - 3ms/step\n",
      "Epoch 66/200\n",
      "24/24 - 0s - loss: 0.0014 - val_loss: 0.0023 - 83ms/epoch - 3ms/step\n",
      "Epoch 67/200\n",
      "24/24 - 0s - loss: 0.0014 - val_loss: 0.0023 - 91ms/epoch - 4ms/step\n",
      "Epoch 68/200\n",
      "24/24 - 0s - loss: 0.0014 - val_loss: 0.0023 - 82ms/epoch - 3ms/step\n",
      "Epoch 69/200\n",
      "24/24 - 0s - loss: 0.0014 - val_loss: 0.0023 - 92ms/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "24/24 - 0s - loss: 0.0014 - val_loss: 0.0022 - 96ms/epoch - 4ms/step\n",
      "Epoch 71/200\n",
      "24/24 - 0s - loss: 0.0013 - val_loss: 0.0022 - 87ms/epoch - 4ms/step\n",
      "Epoch 72/200\n",
      "24/24 - 0s - loss: 0.0013 - val_loss: 0.0022 - 76ms/epoch - 3ms/step\n",
      "Epoch 73/200\n",
      "24/24 - 0s - loss: 0.0013 - val_loss: 0.0022 - 84ms/epoch - 4ms/step\n",
      "Epoch 74/200\n",
      "24/24 - 0s - loss: 0.0013 - val_loss: 0.0022 - 84ms/epoch - 3ms/step\n",
      "Epoch 75/200\n",
      "24/24 - 0s - loss: 0.0013 - val_loss: 0.0022 - 125ms/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 79ms/epoch - 3ms/step\n",
      "Epoch 77/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 75ms/epoch - 3ms/step\n",
      "Epoch 78/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 84ms/epoch - 3ms/step\n",
      "Epoch 79/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 89ms/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 86ms/epoch - 4ms/step\n",
      "Epoch 81/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 83ms/epoch - 3ms/step\n",
      "Epoch 82/200\n",
      "24/24 - 0s - loss: 0.0012 - val_loss: 0.0021 - 78ms/epoch - 3ms/step\n",
      "Epoch 83/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0021 - 79ms/epoch - 3ms/step\n",
      "Epoch 84/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0021 - 77ms/epoch - 3ms/step\n",
      "Epoch 85/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0021 - 80ms/epoch - 3ms/step\n",
      "Epoch 86/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0020 - 77ms/epoch - 3ms/step\n",
      "Epoch 87/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0020 - 77ms/epoch - 3ms/step\n",
      "Epoch 88/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0020 - 75ms/epoch - 3ms/step\n",
      "Epoch 89/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0020 - 78ms/epoch - 3ms/step\n",
      "Epoch 90/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0020 - 73ms/epoch - 3ms/step\n",
      "Epoch 91/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 78ms/epoch - 3ms/step\n",
      "Epoch 92/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 71ms/epoch - 3ms/step\n",
      "Epoch 93/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 83ms/epoch - 3ms/step\n",
      "Epoch 94/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 70ms/epoch - 3ms/step\n",
      "Epoch 95/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 74ms/epoch - 3ms/step\n",
      "Epoch 96/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0020 - 69ms/epoch - 3ms/step\n",
      "Epoch 97/200\n",
      "24/24 - 0s - loss: 9.9785e-04 - val_loss: 0.0020 - 70ms/epoch - 3ms/step\n",
      "Epoch 98/200\n",
      "24/24 - 0s - loss: 9.9073e-04 - val_loss: 0.0020 - 79ms/epoch - 3ms/step\n",
      "Epoch 99/200\n",
      "24/24 - 0s - loss: 9.8400e-04 - val_loss: 0.0020 - 82ms/epoch - 3ms/step\n",
      "Epoch 100/200\n",
      "24/24 - 0s - loss: 9.7765e-04 - val_loss: 0.0020 - 79ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "24/24 - 0s - loss: 9.7169e-04 - val_loss: 0.0020 - 78ms/epoch - 3ms/step\n",
      "Epoch 102/200\n",
      "24/24 - 0s - loss: 9.6613e-04 - val_loss: 0.0020 - 77ms/epoch - 3ms/step\n",
      "Epoch 103/200\n",
      "24/24 - 0s - loss: 9.6097e-04 - val_loss: 0.0020 - 70ms/epoch - 3ms/step\n",
      "Epoch 104/200\n",
      "24/24 - 0s - loss: 9.5621e-04 - val_loss: 0.0020 - 75ms/epoch - 3ms/step\n",
      "Epoch 105/200\n",
      "24/24 - 0s - loss: 9.5186e-04 - val_loss: 0.0020 - 72ms/epoch - 3ms/step\n",
      "Epoch 106/200\n",
      "24/24 - 0s - loss: 9.4793e-04 - val_loss: 0.0020 - 79ms/epoch - 3ms/step\n",
      "Epoch 107/200\n",
      "24/24 - 0s - loss: 9.4441e-04 - val_loss: 0.0020 - 66ms/epoch - 3ms/step\n",
      "Epoch 108/200\n",
      "24/24 - 0s - loss: 9.4132e-04 - val_loss: 0.0020 - 71ms/epoch - 3ms/step\n",
      "Epoch 109/200\n",
      "24/24 - 0s - loss: 9.3865e-04 - val_loss: 0.0020 - 62ms/epoch - 3ms/step\n",
      "Epoch 110/200\n",
      "24/24 - 0s - loss: 9.3641e-04 - val_loss: 0.0020 - 65ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "24/24 - 0s - loss: 9.3461e-04 - val_loss: 0.0020 - 67ms/epoch - 3ms/step\n",
      "Epoch 112/200\n",
      "24/24 - 0s - loss: 9.3325e-04 - val_loss: 0.0020 - 61ms/epoch - 3ms/step\n",
      "Epoch 113/200\n",
      "24/24 - 0s - loss: 9.3233e-04 - val_loss: 0.0020 - 64ms/epoch - 3ms/step\n",
      "Epoch 114/200\n",
      "24/24 - 0s - loss: 9.3186e-04 - val_loss: 0.0020 - 64ms/epoch - 3ms/step\n",
      "Epoch 115/200\n",
      "24/24 - 0s - loss: 9.3183e-04 - val_loss: 0.0019 - 60ms/epoch - 3ms/step\n",
      "Epoch 116/200\n",
      "24/24 - 0s - loss: 9.3225e-04 - val_loss: 0.0019 - 63ms/epoch - 3ms/step\n",
      "Epoch 117/200\n",
      "24/24 - 0s - loss: 9.3312e-04 - val_loss: 0.0019 - 60ms/epoch - 3ms/step\n",
      "Epoch 118/200\n",
      "24/24 - 0s - loss: 9.3444e-04 - val_loss: 0.0019 - 63ms/epoch - 3ms/step\n",
      "Epoch 119/200\n",
      "24/24 - 0s - loss: 9.3621e-04 - val_loss: 0.0019 - 64ms/epoch - 3ms/step\n",
      "Epoch 120/200\n",
      "24/24 - 0s - loss: 9.3842e-04 - val_loss: 0.0018 - 70ms/epoch - 3ms/step\n",
      "Epoch 121/200\n",
      "24/24 - 0s - loss: 9.4109e-04 - val_loss: 0.0018 - 82ms/epoch - 3ms/step\n",
      "Epoch 122/200\n",
      "24/24 - 0s - loss: 9.4423e-04 - val_loss: 0.0018 - 72ms/epoch - 3ms/step\n",
      "Epoch 123/200\n",
      "24/24 - 0s - loss: 9.4784e-04 - val_loss: 0.0018 - 82ms/epoch - 3ms/step\n",
      "Epoch 124/200\n",
      "24/24 - 0s - loss: 9.5195e-04 - val_loss: 0.0017 - 78ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "24/24 - 0s - loss: 9.5661e-04 - val_loss: 0.0017 - 85ms/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "24/24 - 0s - loss: 9.6188e-04 - val_loss: 0.0017 - 81ms/epoch - 3ms/step\n",
      "Epoch 127/200\n",
      "24/24 - 0s - loss: 9.6787e-04 - val_loss: 0.0016 - 83ms/epoch - 3ms/step\n",
      "Epoch 128/200\n",
      "24/24 - 0s - loss: 9.7474e-04 - val_loss: 0.0016 - 82ms/epoch - 3ms/step\n",
      "Epoch 129/200\n",
      "24/24 - 0s - loss: 9.8273e-04 - val_loss: 0.0016 - 83ms/epoch - 3ms/step\n",
      "Epoch 130/200\n",
      "24/24 - 0s - loss: 9.9218e-04 - val_loss: 0.0015 - 84ms/epoch - 3ms/step\n",
      "Epoch 131/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0015 - 90ms/epoch - 4ms/step\n",
      "Epoch 132/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0015 - 87ms/epoch - 4ms/step\n",
      "Epoch 133/200\n",
      "24/24 - 0s - loss: 0.0010 - val_loss: 0.0015 - 102ms/epoch - 4ms/step\n",
      "Epoch 134/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0016 - 101ms/epoch - 4ms/step\n",
      "Epoch 135/200\n",
      "24/24 - 0s - loss: 0.0011 - val_loss: 0.0016 - 93ms/epoch - 4ms/step\n",
      "Epoch 00135: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, epochs=200, batch_size=None, validation_data=(validation_X, validation_y), verbose=2, shuffle=False, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9549f1f",
   "metadata": {},
   "source": [
    "Y axis shows loss which is the mean squared error value while x axis showing epochs. Figure 4 shows comparison of training data and validation data. It shows decrease in the loss by epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba28899-c91a-457c-83b9-fdf6cb47c71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEKCAYAAAAxXHOuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzx0lEQVR4nO3deZxU1bnv/8+3qrtpuqEZG2UQQcUBEAFb5AQj4hTARNQYgyNmOEQTTzQn5ojJiVNeOdebi8bjicNPPRqTmCDXSOQXcQ5OJw6AIoJAQERpQEaZp+6u5/6xdzdF0V29q6miq+nn/XrVq/Zee69VTzXQD2utvdeWmeGcc85lS6y5A3DOOXdo8cTinHMuqzyxOOecyypPLM4557LKE4tzzrms8sTinHMuq3KaWCSNlrRY0lJJk+o5frmkeeHr75JOaqyupM6SXpK0JHzvlMvv4JxzLjM5SyyS4sB9wBigP3CppP4pp30CjDSzQcAvgIci1J0EvGJm/YBXwn3nnHN5Ipc9lmHAUjNbZmZ7gCnAuOQTzOzvZvZFuPs20CtC3XHA4+H248AFufsKzjnnMlWQw7Z7AiuS9iuBU9Oc/x3guQh1DzOz1QBmtlpSt8YC6dq1q/Xp0ydi2M455wDmzJmz3szKM62Xy8SiesrqXT9G0iiCxHJapnUb/HBpIjARoHfv3syePTuT6s451+pJ+rQp9XI5FFYJHJG03wtYlXqSpEHAI8A4M9sQoe4aSd3Dut2BtfV9uJk9ZGYVZlZRXp5xwnXOOddEuUwss4B+kvpKKgLGA9OTT5DUG3gauNLM/hGx7nRgQrg9AXgmh9/BOedchnI2FGZm1ZKuA14A4sCjZrZA0jXh8QeBW4AuwP2SAKrDXka9dcOm7wSmSvoO8BnwjVx9B+ecc5lTa1g2v6KiwnyOxbmDq6qqisrKSnbt2tXcobhGFBcX06tXLwoLC/cplzTHzCoybS+Xk/fOuVassrKS9u3b06dPH8IRCZeHzIwNGzZQWVlJ3759s9KmL+ninMuJXbt20aVLF08qeU4SXbp0yWrP0hOLcy5nPKm0DNn+c/LEkkbNogVUv/ZKc4fhnHMtiieWNBKLF1LzxszmDsM51wSbNm3i/vvvb1LdsWPHsmnTprTn3HLLLbz88stNaj9Vnz59WL9+fVbaygeeWNJQPA41Nc0dhnOuCdIllppG/l3PmDGDjh07pj3njjvu4Oyzz25qeIc0TyzpxOOQ8MTiXEs0adIkPv74YwYPHsxPfvITXn31VUaNGsVll13GiSeeCMAFF1zAySefzIABA3jooYfq6tb2IJYvX84JJ5zAP//zPzNgwADOPfdcdu7cCcDVV1/NU089VXf+rbfeytChQznxxBNZtGgRAOvWreOcc85h6NChfO973+PII49stGdy9913M3DgQAYOHMg999wDwPbt2znvvPM46aSTGDhwIE8++WTdd+zfvz+DBg3ixhtvzOrP70D45cbpxLzH4lw2VP91GonVK7PaZqx7Twq+emGDx++8807mz5/P3LlzAXj11Vd59913mT9/ft1ltY8++iidO3dm586dnHLKKXz961+nS5cu+7SzZMkS/vSnP/Hwww9zySWX8Oc//5krrrhiv8/r2rUr7733Hvfffz+TJ0/mkUce4fbbb+fMM8/k5ptv5vnnn98nedVnzpw5PPbYY7zzzjuYGaeeeiojR45k2bJl9OjRg2effRaAzZs3s3HjRqZNm8aiRYuQ1OjQ3cHkPZZ04jFIJGgNN5E61xoMGzZsn3s17r33Xk466SSGDx/OihUrWLJkyX51+vbty+DBgwE4+eSTWb58eb1tX3TRRfud8+abbzJ+/HgARo8eTadO6Z9L+Oabb3LhhRdSWlpKu3btuOiii3jjjTc48cQTefnll7npppt444036NChA2VlZRQXF/Pd736Xp59+mpKSkgx/GrnjPZZ0YvHgPZEIhsWcc02SrmdxMJWWltZtv/rqq7z88su89dZblJSUcMYZZ9R7L0ebNm3qtuPxeN1QWEPnxeNxqqurATL+T2lD5x977LHMmTOHGTNmcPPNN3Puuedyyy238O677/LKK68wZcoUfvOb3/C3v/0to8/LFe+xpFObTHw4zLkWp3379mzdurXB45s3b6ZTp06UlJSwaNEi3n777azHcNpppzF16lQAXnzxRb744ou0559++un85S9/YceOHWzfvp1p06bx5S9/mVWrVlFSUsIVV1zBjTfeyHvvvce2bdvYvHkzY8eO5Z577qkb8ssH3mNJpzax+AS+cy1Oly5dGDFiBAMHDmTMmDGcd955+xwfPXo0Dz74IIMGDeK4445j+PDhWY/h1ltv5dJLL+XJJ59k5MiRdO/enfbt2zd4/tChQ7n66qsZNmwYAN/97ncZMmQIL7zwAj/5yU+IxWIUFhbywAMPsHXrVsaNG8euXbswM379619nPf6m8kUo06j5++tU/3UaRT/7BSptl4PInDt0LVy4kBNOOKG5w2hWu3fvJh6PU1BQwFtvvcW1116bVz2LZPX9efkilLngQ2HOuQPw2Wefcckll5BIJCgqKuLhhx9u7pAOCk8s6cSTJu+dcy5D/fr14/3332/uMA46n7xPJ+Y9Fuecy5QnlnTCHot5YnHOuchymlgkjZa0WNJSSZPqOX68pLck7ZZ0Y1L5cZLmJr22SLohPHabpJVJx8bm7AvEwh+PXxXmnHOR5WyORVIcuA84B6gEZkmabmYfJZ22EfghcEFyXTNbDAxOamclMC3plF+b2eRcxV7HJ++dcy5jueyxDAOWmtkyM9sDTAHGJZ9gZmvNbBZQlaads4CPzezT3IXaAJ+8d65VadcuuK1g1apVXHzxxfWec8YZZ9DY7Qv33HMPO3bsqNuPsgx/FLfddhuTJ+f+/9QHKpeJpSewImm/MizL1HjgTyll10maJ+lRSekX3zkA8sl751qlHj161K1c3BSpiSXKMvyHklwmlvqedZnR3ZiSioDzgf+bVPwAcDTBUNlq4K4G6k6UNFvS7HXr1mXysXv5UJhzLdZNN920z/NYbrvtNu666y62bdvGWWedVbfE/TPPPLNf3eXLlzNw4EAAdu7cyfjx4xk0aBDf/OY391kr7Nprr6WiooIBAwZw6623AsHClqtWrWLUqFGMGjUK2PdBXvUti59uef6GzJ07l+HDhzNo0CAuvPDCuuVi7r333rql9GsXwHzttdcYPHgwgwcPZsiQIWmXusmGXN7HUgkckbTfC1iVYRtjgPfMbE1tQfK2pIeBv9ZX0cweAh6C4M77DD83EPfJe+ey4cdLP2Hetu1ZbXNQu1LuOqZvg8fHjx/PDTfcwPe//30Apk6dyvPPP09xcTHTpk2jrKyM9evXM3z4cM4///wGn/v+wAMPUFJSwrx585g3bx5Dhw6tO/bLX/6Szp07U1NTw1lnncW8efP44Q9/yN13383MmTPp2rXrPm01tCx+p06dIi/PX+uqq67iv/7rvxg5ciS33HILt99+O/fccw933nknn3zyCW3atKkbfps8eTL33XcfI0aMYNu2bRQXF0f9MTdJLnsss4B+kvqGPY/xwPQM27iUlGEwSd2Tdi8E5h9QlOnE/HJj51qqIUOGsHbtWlatWsUHH3xAp06d6N27N2bGT3/6UwYNGsTZZ5/NypUrWbNmTYPtvP7663W/4AcNGsSgQYPqjk2dOpWhQ4cyZMgQFixYwEcffdRQM0DDy+JD9OX5IVhAc9OmTYwcORKACRMm8Prrr9fFePnll/OHP/yBgoKg7zBixAj+9V//lXvvvZdNmzbVledKzlo3s2pJ1wEvAHHgUTNbIOma8PiDkg4HZgNlQCK8pLi/mW2RVEJwRdn3Upr+laTBBMNqy+s5nj0+ee9cVqTrWeTSxRdfzFNPPcXnn39eNyz0xBNPsG7dOubMmUNhYSF9+vSpd7n8ZPX1Zj755BMmT57MrFmz6NSpE1dffXWj7aRbmzHq8vyNefbZZ3n99deZPn06v/jFL1iwYAGTJk3ivPPOY8aMGQwfPpyXX36Z448/vkntR5HT+1jMbIaZHWtmR5vZL8OyB83swXD7czPrZWZlZtYx3N4SHtthZl3MbHNKm1ea2YlmNsjMzjez1Tn7Aj5571yLNn78eKZMmcJTTz1Vd5XX5s2b6datG4WFhcycOZNPP01/wenpp5/OE088AcD8+fOZN28eAFu2bKG0tJQOHTqwZs0annvuubo6DS3Z39Cy+Jnq0KEDnTp1quvt/P73v2fkyJEkEglWrFjBqFGj+NWvfsWmTZvYtm0bH3/8MSeeeCI33XQTFRUVdY9OzhVfKywdn7x3rkUbMGAAW7dupWfPnnTvHoyiX3755Xzta1+joqKCwYMHN/o/92uvvZZvfetbDBo0iMGDB9ctaX/SSScxZMgQBgwYwFFHHcWIESPq6kycOJExY8bQvXt3Zs6cWVfe0LL46Ya9GvL4449zzTXXsGPHDo466igee+wxampquOKKK9i8eTNmxo9+9CM6duzIz3/+c2bOnEk8Hqd///6MGTMm48/LhC+bn4ZtXM+eyb+k4OJLiQ8dloPInDt0+bL5LUs2l833tcLSqRsK8zkW55yLyhNLOj4U5pxzGfPEkk7t6sZ+H4tzTdIahtoPBdn+c/LEko5fFeZckxUXF7NhwwZPLnnOzNiwYUNWb5r0q8LS8TvvnWuyXr16UVlZSZOXVHIHTXFxMb169cpae55Y0vHJe+earLCwkL59m+fGSNe8fCgsHX/Ql3POZcwTSxqSggl877E451xknlgaE4t7j8U55zLgiaUx8ZhfFeaccxnwxNKYWNyXzXfOuQx4YmlMPOZDYc45lwFPLI2J+eS9c85lwhNLY+I+ee+cc5nwxNIIxeI+ee+ccxnIaWKRNFrSYklLJU2q5/jxkt6StFvSjSnHlkv6UNJcSbOTyjtLeknSkvC9Uy6/Q3AfiycW55yLKmeJRVIcuA8YA/QHLpXUP+W0jcAPgckNNDPKzAanPGhmEvCKmfUDXgn3c8cn751zLiO57LEMA5aa2TIz2wNMAcYln2Bma81sFlCVQbvjgMfD7ceBC7IQa8Niccwn751zLrJcJpaewIqk/cqwLCoDXpQ0R9LEpPLDzGw1QPje7YAjTceHwpxzLiO5XN1Y9ZRl8mCGEWa2SlI34CVJi8zs9cgfHiSjiQC9e/fO4GNT+JIuzjmXkVz2WCqBI5L2ewGrolY2s1Xh+1pgGsHQGsAaSd0Bwve1DdR/yMwqzKyivLy8CeGHfEkX55zLSC4Tyyygn6S+koqA8cD0KBUllUpqX7sNnAvMDw9PByaE2xOAZ7IadWosfh+Lc85lJGdDYWZWLek64AUgDjxqZgskXRMef1DS4cBsoAxISLqB4AqyrsA0SbUx/tHMng+bvhOYKuk7wGfAN3L1HQC/89455zKU0ydImtkMYEZK2YNJ258TDJGl2gKc1ECbG4Czshhmej5575xzGfE77xvjk/fOOZcRTyyNicd82XznnMtA2sQiKSbpSwcrmLwUj0PC51iccy6qtInFzBLAXQcplvzki1A651xGogyFvSjp6wov0Wpt5JP3zjmXkShXhf0rUArUSNpJcEe9mVlZTiPLF34fi3POZaTRxGJm7Q9GIHkr5nfeO+dcJiLdxyLpfOD0cPdVM/tr7kLKMz5575xzGWl0jkXSncD1wEfh6/qwrHWIxcEM8+TinHORROmxjAUGh1eIIelx4H1y/YCtfBGPB+81NcGwmHPOubSi/qbsmLTdIQdx5K/axOIT+M45F0mUHst/AO9LmklwRdjpwM05jSqf1PZSfALfOeciSZtYJMWABDAcOIUgsdwULh7ZKqhuKMznWJxzLoq0icXMEpKuM7OpRHyWyiHHh8Kccy4jUeZYXpJ0o6QjJHWufeU8snwRS5q8d84516gocyzfDt9/kFRmwFHZDycPhT0WS9TQKte0cc65DEWZY5lkZk8epHjyj0/eO+dcRqKsbvyDdOekI2m0pMWSlkra774XScdLekvSbkk3JpUfIWmmpIWSFki6PunYbZJWSpobvsY2Nb5IfPLeOecyEmUo7KXwl/6TwPbaQjPbmK6SpDhwH3AOUAnMkjTdzD5KOm0j8EPggpTq1cCPzew9Se2BOZJeSqr7azObHCH2A+eT9845l5FczrEMA5aa2TIASVOAcQTLwgSNmK0F1ko6L7mima0GVofbWyUtBHom1z1Y5JP3zjmXkSirG/dtYts9gRVJ+5XAqZk2IqkPMAR4J6n4OklXAbMJejZfNDHGxsU9sTjnXCYanGOR9G9J299IOfYfEdqu7yIqix4aSGoH/Bm4wcy2hMUPAEcDgwl6NfU+4VLSREmzJc1et25dJh+7r9rJex8Kc865SNJN3o9P2k5dwmV0hLYrgSOS9nsBqyLGhaRCgqTyhJk9XVtuZmvMrCa8sOBhgiG3/ZjZQ2ZWYWYV5eXlUT92f7WXG/vkvXPORZIusaiB7fr26zML6Cepr6QigkQV6e798DHI/w0sNLO7U451T9q9EJgfpc0m88l755zLSLo5Fmtgu779/SubVUu6DngBiAOPmtkCSdeExx+UdDjBPEkZkJB0A9AfGARcCXwoaW7Y5E/NbAbwK0mDwxiWA99rLJYD4pP3zjmXkXSJ5SRJWwh6J23DbcL94iiNh4lgRkrZg0nbnxMMkaV6kwZ6RWZ2ZZTPzhqfvHfOuYw0mFjMLH4wA8lXivvkvXPOZcIfidiYmN9575xzmfDE0hgfCnPOuYx4YmlMzK8Kc865THhiaUzdfSyeWJxzLooGJ+8lbSXNZcVmVpaTiPKNT94751xG0l0V1h5A0h3A58DvCS4Bvhxof1Ciywc+ee+ccxmJMhT2FTO738y2mtkWM3sA+HquA8sbPnnvnHMZiZJYaiRdLikuKSbpcqDV/JaVFCxEmfAei3PORRElsVwGXAKsCV/fCMtaj1jc51iccy6iKM9jWU7wgK7WKx7zoTDnnIuo0R6LpGMlvSJpfrg/SNK/5z60PBKL++XGzjkXUZShsIcJnsdSBWBm89j3WS2HvrgPhTnnXFRREkuJmb2bUladi2DyVjzulxs751xEURLLeklHE94sKeligkcCtx6xmPdYnHMuokYn74EfAA8Bx0taCXxCcJNkq6F43CfvnXMuorSJRVIcuNbMzpZUCsTMbOvBCS2PxDyxOOdcVGmHwsysBjg53N6eaVKRNFrSYklLJU2q5/jxkt6StFvSjVHqSuos6SVJS8L3TpnE1CTxuN8g6ZxzEUWZY3lf0nRJV0q6qPbVWKWwt3MfMIbgOfaXSuqfctpG4IfA5AzqTgJeMbN+wCvhfm75UJhzzkUWJbF0BjYAZwJfC19fjVBvGLDUzJaZ2R5gCik3WprZWjObRXgpc8S644DHw+3HgQsixHJgYjHMJ++dcy6SKHfef6uJbfcEViTtVwKnZqHuYWa2OoxttaRuTYwvOu+xOOdcZI0mFknFwHeAAUBxbbmZfbuxqvWUNfh8lyzWDRqQJgITAXr37p1J1f3F47Bnz4G14ZxzrUSUobDfA4cDXwFeA3oBUSbxK4EjkvZ7AasixpWu7hpJ3QHC97X1NWBmD5lZhZlVlJeXR/zY+inmk/fOORdVlMRyjJn9HNhuZo8D5wEnRqg3C+gnqa+kIoJlYKZHjCtd3enAhHB7AvBMxDabzofCnHMusig3SNZOrG+SNJDgaZJ9GqtkZtWSrgNeAOLAo2a2QNI14fEHJR0OzAbKgISkG4D+Zralvrph03cCUyV9B/iMYBn/3Ir56sbOORdVlMTyUHivyM8JegvtgFuiNG5mM4AZKWUPJm1/TjDMFaluWL4BOCvK52eNL0LpnHORRbkq7JFw8zXgqNyGk6fivmy+c85FFeWqsHp7J2Z2R/bDyVM+ee+cc5FFGQrbnrRdTHBz5MLchJOn/AmSzjkXWZShsLuS9yVNJvrVXYcGX4TSOecii3K5caoSWtlci3zy3jnnIosyx/Ihe+96jwPlQOuZXwF/gqRzzmUgyhxL8oKT1cAaM2tdjyaOeY/FOeeiipJYUpdvKZP2LuVlZhuzGlE+iscgkcDMSP7uzjnn9hclsbxHsG7XFwSLQ3YkuOMdgiGyQ3++JR4P3mtqoCDKj8w551qvKJP3zwNfM7OuZtaFYGjsaTPra2aHflKBYEkX8OEw55yLIEpiOSVcXgUAM3sOGJm7kPJQrLbH4hP4zjnXmCjjOusl/TvwB4KhrysInijZaih5KMw551xaUXoslxJcYjwN+AvQLSxrPWp7LD4U5pxzjYpy5/1G4HqAcJXjTWaW0dMcWzzvsTjnXGQN9lgk3SLp+HC7jaS/AUsJnuB49sEKMC/EffLeOeeiSjcU9k1gcbg9ITy3G8HE/X/kOK78Eg6FmU/eO+dco9Illj1JQ15fAf5kZjVmtpBok/6HDh8Kc865yNIllt2SBkoqB0YBLyYdK4nSuKTRkhZLWippUj3HJene8Pg8SUPD8uMkzU16bQkfW4yk2yStTDo2NvK3bSIVFwcbu3bm+qOcc67FS9fzuB54iuCKsF+b2ScA4S/y9xtrWFIcuA84B6gEZkmabmYfJZ02BugXvk4FHgBONbPFwOCkdlYSXJVW69dmNjnKF8yK9mUA2NYtB+0jnXOupWowsZjZO8Dx9ZTX+yz6egwDlprZMgBJU4BxQHJiGQf8Lhxye1tSR0ndzWx10jlnAR+b2acRPjMnVJtYtmxurhCcc67FaMrzWKLqCaxI2q8MyzI9Zzzwp5Sy68Khs0fDS6Bzq21JsEaY91icc65RuUws9S0DnHr/S9pzJBUB5wP/N+n4A8DRBENlq4F9nnCZVHeipNmSZq9bty6DsOttC9qX+VCYc85FkMvEUkmwKnKtXsCqDM8ZA7xnZmtqC8xsTXh1WgJ4mGDIbT9m9pCZVZhZRXl5+QF8jYA8sTjnXCSRLhuW9CWgT/L5Zva7RqrNAvpJ6ksw+T4euCzlnOkEw1pTCCbvN6fMr1xKyjBYyhzMhcD8KN/hQKl9GbZuTeMnOudcKxfl0cS/Jxh6mgvU3shhQNrEYmbVkq4DXiB4pPGjZrZA0jXh8QcJLgIYS3BH/w7gW0mfW0JwRdn3Upr+laTBYQzL6zmeG+3LsI+XHJSPcs65lixKj6UC6N+U9cHqu4IsTCi12wb8oIG6O4Au9ZRfmWkc2aCyMti1E6vagwqLmiME55xrEaLMscwHDs91IPmu9pJjtqY+qdk551yyKD2WrsBHkt4FdtcWmtn5OYsqDynpJkl13q8j5ZxzLhQlsdyW6yBaBL9J0jnnIonyPJbXDkYg+W7vUJhfcuycc+k0OsciabikWZK2SdojqUZS6/vtWlIKsRjmcyzOOZdWlMn73xDcT7IEaAt8NyxrVRSL+d33zjkXQaQbJM1sqaS4mdUAj0n6e47jykt+971zzjUuSmLZEa7ZNVfSrwjW5yrNbVj5Se3LsC82NHcYzjmX16IMhV0ZnncdsJ1gba+v5zKovNW+PbbFeyzOOZdOlKvCPpXUFuhuZrcfhJjyltqXwY7tWHU1KmhdT2d2zrmoolwV9jWCdcKeD/cHS5qe47jykso6BBvb/Mow55xrSJShsNsIlqbfBGBmcwlWOm59/BHFzjnXqCiJpdrM/HZzgHbtubvvcSz5YlNzR+Kcc3kr0iKUki4D4pL6SfovoFVebvxOrJDbjh3EA1u2NXcozjmXt6Ikln8BBhAsQPknYAtwQw5jylt/2LodgP/ZXdXMkTjnXP6KclXYDuBn4avV2pVI8NS6DRRagg/jhWyurqaDXxnmnHP7afA3Y2NXfrWGZfOX7tzJgu07GNe1C89u2MjmmhpuVILJFPDW6s8ZfUSv5g7ROefyTrr/cv8TsIJg+OsdQJk2Lmk08J8EjyZ+xMzuTDmu8PhYgkcTX21m74XHlgNbCR6HXG1mFWF5Z+BJgivTlgOXmNkXmcYWxX98WskTa9ZxR5/evLVlK73aFPFvPXtyz9IVvLFypScW55yrR7o5lsOBnwIDCX75nwOsN7PXoiylLykO3AeMAfoDl0rqn3LaGKBf+JoIPJByfJSZDa5NKqFJwCtm1g94JdzPifuPPZrx3bpyy/LPeG7jF1zarZyynkdw0rbN/H3bjlx9rHPOtWgNJhYzqzGz581sAjAcWAq8KulfIrY9DFhqZsvMbA8wBRiXcs444HcWeBvoKKl7I+2OAx4Ptx8HLogYT8aKYzF+e3w/bu/Tm8MKC5lweDcUi/ElapgTL2R3IpGrj3bOuRYr7VVhktpIugj4A/AD4F7g6Yht9yQYSqtVGZZFPceAFyXNkTQx6ZzDzGw1QPjeLWI8TSKJSUf24tN/qqBfSVsARnTowO5YjFmff57Lj3bOuRYp3eT94wTDYM8Bt5vZ/Azbrm9OxjI4Z4SZrZLUDXhJ0iIzez3yhwfJaCJA7969o1ZL117d9og+R8Inq3jzsxWc1qPHAbftnHOHknQ9liuBY4Hrgb9L2hK+tkZ8gmQlwUrItXoBq6KeY2a172uBaQRDawBraofLwve19X24mT1kZhVmVlFeXh4h3OjKe/XmuO1beX2r3yjpnHOp0s2xxMysffgqS3q1N7OyCG3PAvpJ6hs+z2U8kHoJ83TgKgWGA5vNbLWkUkntASSVAucC85PqTAi3JwDPRP62WaJYjHOsmjcK2rB1z56D/fHOOZfXotx53yRmVk3wDJcXgIXAVDNbIOkaSdeEp80AlhFcGPAw8P2w/DDgTUkfAO8Cz5rZ8+GxO4FzJC0huFJtn0uYD5avHtGLPbEYLy7IdITQOecObTJLnfY49FRUVNjs2bOz2mZVdTW9Xn2T83bv4NHzxma1beecyweS5qTc7hFJznosh7rCggJGy3iuqC1VW3zxZ+ecq+WJ5QB89cjebCxqw9/nvt/coTjnXN7wxHIAvtLnSAotwV8/X0NrGFJ0zrkoPLEcgLKCAkYWxHi2XUcSHy9p7nCccy4veGI5QF/v04ePS9vz2uxZzR2Kc87lBU8sB+jS7ofR1RLcG29DYl2992o651yr4onlALWNx/le98N5rlsPFr79P80djnPONTtPLFlwTd/etDHjN1t3YDu2N3c4zjnXrDyxZEG3oiIu69iePx7em8/fjLxOpnPOHZI8sWTJDccew55YjP+9dj223RendM61Xp5YsuT4khK+1akDD/fsy6I3Xm3ucJxzrtl4YsmiW084lmLg37ftwrZGebKAc84dejyxZNFhRUXceHg5fy3vwczX/tbc4TjnXLPwxJJlNxx7DL0TNVwXa8vWTz5u7nCcc+6g88SSZW3jcR4ecALLStpx05y5WHV1c4fknHMHlSeWHDijW1euL23Df5d359k3/PJj51zr4oklR+44eSgDq3bz3Wqx6B+Lmzsc55w7aHKaWCSNlrRY0lJJk+o5Lkn3hsfnSRoalh8haaakhZIWSLo+qc5tklZKmhu+8vLxjW1iMaZWDKFA4vxPVrB6/frmDsk55w6KnCUWSXHgPmAM0B+4VFL/lNPGAP3C10TggbC8GvixmZ0ADAd+kFL312Y2OHzNyNV3OFBHd+zItGP6sK6wiHGz32fD9h3NHZJzzuVcLnssw4ClZrbMzPYAU4BxKeeMA35ngbeBjpK6m9lqM3sPwMy2AguBnjmMNWdO6dOHJzqWsrBNMWf9z1tUbvW78p1zh7ZcJpaewIqk/Ur2Tw6NniOpDzAEeCep+Lpw6OxRSZ2yFnGOjB16Mn8pLWJFvIAz35nFwk2bmzsk55zLmVwmFtVTlvr83rTnSGoH/Bm4wcxqb2V/ADgaGAysBu6q98OliZJmS5q9bt26DEPPvrOGncrzZcXsSBinvTePp5d/2twhOedcTuQysVQCRyTt9wJWRT1HUiFBUnnCzJ6uPcHM1phZjZklgIcJhtz2Y2YPmVmFmVWUl5cf8JfJhlNOOZX/6VnOCds2c+mnK/nxnPfZlUg0d1jOOZdVuUwss4B+kvpKKgLGA9NTzpkOXBVeHTYc2GxmqyUJ+G9goZndnVxBUvek3QuB+bn7Ctl35IATeWlYBdesreQ323Yy/NU3ef+LTc0dlnPOZU3OEouZVQPXAS8QTL5PNbMFkq6RdE142gxgGbCUoPfx/bB8BHAlcGY9lxX/StKHkuYBo4Af5eo75Erbww/nnnHjmLZ9I19UVTHig/n8eNZsNvtd+s65Q4DMUqc9Dj0VFRU2e/bs5g6jXuuXLObnH3zIY127062mmv/VqzuXHduPoNPmnHPNR9IcM6vItJ7fed/MuvY7jvsvuIDXY1X03LGNb3++njNfeoVZPrnvnGuhPLHkARUUMOz0M3jzrDO4r2oHixTntE9X8vUXXuKDFSsab8A55/KIJ5Y8Ei8p5btnn82iU4fy86odvBEvYtiyFYyf8TwLlvyjucNzzrlIPLHkoQ5lHfj3s89mUcVJ3FSzi5falDB01Xou+OuzvDb3fRI1Nc0donPONcgTSx7r3LEjd5x5JouGn8xPYwneLS7l3M07Oe2Fl5j6979TtWd3c4fonHP78cTSApSXlnLrl09jyelf4j9LCtlUUMiVVTBg5hvc+9JLbN2woblDdM65Op5YWpDSoiKuOeUU5p09iie7dKCH4CdFpRz1/nx++OyzzF+8iNZw+bhzLr95YmmBCmIxLhg4gFfPPZvXjj6SsUrwWEkZJ3++kTNnPM8f33qLXbt2NneYzrlWym+QPESs27Gd330wj0e27mBZm7Z02bObqxJ7+Ha/Y+jX9yi/4dI5l7Gm3iDpieUQU5NI8LfFi3loRSXPFralJhajYvtWLisr5ZJBJ1Fe1r65Q3TOtRCeWNJoTYkl2cotW5gy70P+uG0H89uWUpBIcG7VTi7sVs7Y44+na3Fxc4fonMtjnljSaK2JJdkHyz7miSUf8xRxVha3JW4JRlTv4fzyrow95hiOLi1p7hCdc3nGE0sanlj2SlRVMXvhRzyzopL/XwUsLg2Gxo6sqWJk22JGHXEEZ5R3pUebomaO1DnX3DyxpOGJpX62ZzeLF8zn5cpKXt1TwxsdOrOpMEgovSzBkJJiTi7vyskdOjCoXSmHFRb6RQDOtSKeWNLwxNI4q6mh+tNPmLvkH7yx4Qvei8V5v6wTS0v3TvaXAf2K29CvfTuOKy3hiDZt6NGmiB5FRfRsU0RZQUHzfQHnXNY1NbH4bwIHgOJxCo86hlOOOoZTANu5g8Snn7Dpk2W8v3YtC3bvZknbdiwtac+b7dozpXj/OZn2sRg92rShe5siuhQW0LmggM6FBXQuKAzfg/1O4XtZPE5xLOa9IOcOMZ5YXL3UtoT48QPocvwAzgbOqqnB1q3BVq/EVq9i+8dLWLllK6urqlhV3JbVbdoG78WlrC4p5cOiNmwsKOSLWJyaNIkjDrQviNM+HqddPOk9qaws3G8Xj1Maj9E2FiSktuGrOCbaxmMUx+L7lHnScq555DSxSBoN/CfB749HzOzOlOMKj48FdgBXm9l76epK6gw8CfQBlgOXmNkXufweLujR6PAecHgPGAIdCF4nVO3BNmzANq7HNm6ArVuwLRuw1Vtg62YSWzazJZHgi8IiNha2Cd+L+KKwiG0FBWwrLGJrm2K2FRWzrbYsXsDn8TjbYnG2KcZWiaomJojimMJkE6dNmGwKJQpjokjBdoFEUUwUhvvBdviqPX+f8hgFEnFBXApeBNuxpO147TkoPLZ3u64uYXm4ndqmBEIoZTu233aEYwqW2mh4G6SwXj3bzkWVs8QiKQ7cB5wDVAKzJE03s4+SThsD9AtfpwIPAKc2UncS8IqZ3SlpUrh/U66+h0tPhUXo8O5wePd6j5sZ5bt3Ub59O7ZzB+zcufd9V/L+FmzLLtizp+5lVXtgz27Ys4fdRl3S2RWPsysWZ1c8zs5YnJ3hfu37rlis7tiueJyd8QJ2xQvYWVDInnicqnic6liMPbEYVYqxK3yvChNYVSzcRuyRqA7L96C0va/WIDXp1L6op2yfpNjg/r6JE/YvU1IZKfupx/dtY99k2fB+SlzsTaSN1SPC96s9q7Ysxr7/sUj9z0Qs6T8ldefW/YckdT843wwMI2FQE74nMGoMrjisnH4lbQ/sDz5DueyxDAOWmtkyAElTgHFAcmIZB/zOgisI3pbUUVJ3gt5IQ3XHAWeE9R8HXsUTS96SBMVtobht3T+ypiiqrqZ9VZhwdu+Gqj1QUwNVVVBTjVVXQ+2rphqqqrCamrCsqu5YcN6uYD+RCNpIJLBEDdQkIFGzT/k+xxMJEjU1VJtRlTBqzKixBAkLtyUSYfKpfSUkalLKUssTYu8xwmPhyxAGmMAIzq0tSyh4J2nbECZIJNVLhGWN1UtuO7VeArCUz6jdJvlY8nt9ZSntkHReXTtK/ozUOqTEsDdekuNJ+pnV1UltI02bteprJ7HP99+/HZKO18aUfDwh9v6dgJS/NyT9vQh+7jXa9+9QIoP/3MTM+Ketm+g3aFDkOtmQy8TSE0h+rm4lQa+ksXN6NlL3MDNbDWBmqyV1y2bQLj+poAAKCqBtyQElqFyxRALMgkRkCUhY0nu4beF2eK4ln2e273bSeXXv4TGLeB6J8FeZEb7Xvtin3GrLYe85tdu156VuW2rb1FOW2hZhbHv39/vsfeoltbXfDzyl2PbbSDqWwZWv2Ti3wSYs7MYoqXtWu13bvVFSV2ffc6XaRMy+/4EJj8UlYuwdWq3t4cV6HBv9O2VJLhNLff/+U3/kDZ0TpW76D5cmAhMBevfunUlV5zKmWLhQeDwevU6OYnGuueVy2fxK4Iik/V7AqojnpKu7JhwuI3xfW9+Hm9lDZlZhZhXl5eVN/hLOOecyk8vEMgvoJ6mvpCJgPDA95ZzpwFUKDAc2h8Nc6epOByaE2xOAZ3L4HZxzzmUoZ0NhZlYt6TrgBYJLhh81swWSrgmPPwjMILjUeCnB5cbfSlc3bPpOYKqk7wCfAd/I1XdwzjmXOV/SxTnnXL2auqSLP5rYOedcVnlicc45l1WeWJxzzmWVJxbnnHNZ1Som7yWtAz5tYvWuwPoshnOwtMS4PeaDpyXG3RJjhpYZd23MR5pZxjcCtorEciAkzW7KVRHNrSXG7TEfPC0x7pYYM7TMuA80Zh8Kc845l1WeWJxzzmWVJ5bGPdTcATRRS4zbYz54WmLcLTFmaJlxH1DMPsfinHMuq7zH4pxzLqs8saQhabSkxZKWho9BzjuSjpA0U9JCSQskXR+Wd5b0kqQl4Xun5o41laS4pPcl/TXcbwkxd5T0lKRF4c/8n/I9bkk/Cv9uzJf0J0nF+RizpEclrZU0P6mswTgl3Rz+21ws6St5FPP/Cf9+zJM0TVLHfIo5jGO/uJOO3SjJJHVNKssobk8sDZAUB+4DxgD9gUsl9W/eqOpVDfzYzE4AhgM/COOcBLxiZv2AV8L9fHM9sDBpvyXE/J/A82Z2PHASQfx5G7eknsAPgQozG0iwWvh48jPm3wKjU8rqjTP8Oz4eGBDWuT/8N3uw/Zb9Y34JGGhmg4B/ADdDXsUM9ceNpCOAcwhWjq8tyzhuTywNGwYsNbNlZrYHmAKMa+aY9mNmq83svXB7K8Evup4EsT4envY4cEGzBNgASb2A84BHkorzPeYy4HTgvwHMbI+ZbSLP4yZ4PEZbSQVACcFD8/IuZjN7HdiYUtxQnOOAKWa228w+IXj0xrCDEWey+mI2sxfNrDrcfZvgQYWQJzGHMdb3swb4NfBv7PvE3ozj9sTSsJ7AiqT9yrAsb0nqAwwB3gEOCx+aRvjerRlDq889BH+BE0ll+R7zUcA64LFwCO8RSaXkcdxmthKYTPA/0NUED9N7kTyOOUVDcbaUf5/fBp4Lt/M6ZknnAyvN7IOUQxnH7YmlYfU9kjxvL6GT1A74M3CDmW1p7njSkfRVYK2ZzWnuWDJUAAwFHjCzIcB28mMIqUHhnMQ4oC/QAyiVdEXzRpUVef/vU9LPCIaqn6gtque0vIhZUgnwM+CW+g7XU5Y2bk8sDasEjkja70UwhJB3JBUSJJUnzOzpsHiNpO7h8e7A2uaKrx4jgPMlLScYYjxT0h/I75gh+DtRaWbvhPtPESSafI77bOATM1tnZlXA08CXyO+YkzUUZ17/+5Q0AfgqcLntvacjn2M+muA/Hx+E/y57Ae9JOpwmxO2JpWGzgH6S+koqIpi8mt7MMe1HkgjG/Bea2d1Jh6YDE8LtCcAzBzu2hpjZzWbWy8z6EPxc/2ZmV5DHMQOY2efACknHhUVnAR+R33F/BgyXVBL+XTmLYB4un2NO1lCc04HxktpI6gv0A95thvj2I2k0cBNwvpntSDqUtzGb2Ydm1s3M+oT/LiuBoeHf+czjNjN/NfACxhJc1fEx8LPmjqeBGE8j6JbOA+aGr7FAF4KraJaE752bO9YG4j8D+Gu4nfcxA4OB2eHP+y9Ap3yPG7gdWATMB34PtMnHmIE/EcwDVYW/2L6TLk6CoZuPgcXAmDyKeSnBnETtv8cH8ynmhuJOOb4c6NrUuP3Oe+ecc1nlQ2HOOeeyyhOLc865rPLE4pxzLqs8sTjnnMsqTyzOOeeyyhOLO6SEq7LelbR/o6TbstT2byVdnI22Gvmcb4QrJ89MKe8jaaekuUmvq7L4uWcoXGnauQNR0NwBOJdlu4GLJP0vM1vf3MHUkhQ3s5qIp38H+L6Zzazn2MdmNjh7kTmXfd5jcYeaaoLHqv4o9UBqj0PStvD9DEmvSZoq6R+S7pR0uaR3JX0o6eikZs6W9EZ43lfD+vHwGRyzwmdwfC+p3ZmS/gh8WE88l4btz5f0v8OyWwhuen1Q0v+J+qUlbZN0l6T3JL0iqTwsHyzpbe19NkinsPwYSS9L+iCsU/sd22nv82aeCO/WJ/yZfBS2MzlqXK6Vau67bf3lr2y+gG1AGcGdwx2AG4HbwmO/BS5OPjd8PwPYBHQnuCt9JXB7eOx64J6k+s8T/IesH8Edy8XARODfw3PaENyZ3zdsdzvQt544exAst1JOMHLwN+CC8NirBM9PSa3TB9jJ3ju65wJfDo8ZwbpUECwk+Jtwex4wMty+I+m7vANcGG4XEyynfwawmWAtqBjwFkGS60xwx3XtDdUdm/vP2V/5/fIeizvkWLC68+8IHnAV1SwLnm2zm2DpihfD8g8JfqHXmmpmCTNbAiwDjgfOBa6SNJfgF3YXgsQD8K4Fz7BIdQrwqgWLQ9augHt6hDg/NrPBSa83wvIE8GS4/QfgNEkdCJLAa2H548DpktoDPc1sGoCZ7bK9a1q9a2aVZpYgSFx9gC3ALuARSRcByetfObcfTyzuUHUPwVxFaVJZNeHf+XCIpyjp2O6k7UTSfoJ95yJT10AygmXF/yXpl31fC555AkGPpT71LUWeTenWakr32ck/hxqgIEx8wwhW0L6AoNfmXIM8sbhDkpltBKYSJJday4GTw+1xQGETmv6GpFg4J3EUwRDRC8C14eMLkHRs+ACwdN4BRkrqquAxr5cCrzVSJ50YUDt/dBnwppltBr6Q9OWw/ErgtbBHVynpgjDeNuHzOOql4Fk/HcxsBnADwUKczjXIrwpzh7K7gOuS9h8GnpH0LsFKuQ31JtJZTJAADgOuMbNdkh4hGDJ6L+wJraORR/2a2WpJNwMzCXoQM8wsytL1R4dDbrUeNbN7Cb7LAElzCOZJvhken0BwIUAJwdDdt8LyK4H/T9IdBCvcfiPNZ7Yn+LkVh7Hud2GEc8l8dWPnDgGStplZu+aOwznwoTDnnHNZ5j0W55xzWeU9Fuecc1nlicU551xWeWJxzjmXVZ5YnHPOZZUnFuecc1nlicU551xW/T+BglulMRhGJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='training loss', color='#f87970')\n",
    "pyplot.plot(history.history['val_loss'], label='validation loss', color='#06c0c5')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"Number of Epochs\")\n",
    "pyplot.ylabel(\"Mean Squared Error\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b48ec7",
   "metadata": {},
   "source": [
    "LSTM model makes prediction with help of predict function in the Keras library. Since the model takes scaled values, it also makes the estimation in this value range. Therefore, back scaling is needed to obtain true value. But, because of dealing with economic data, only the inverse transformation of min-max scaling is taken. Thus, in this case, values are still logarithmed. After taken back scaling of prediction and tesy_y values, root of the mean squared error is calculate value 0.59 as shown Figure **???**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a61968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.412\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(test_X)\n",
    "\n",
    "inv_yhat = min_max_scaler_y.inverse_transform(yhat) \n",
    "test_y = test_y.reshape((len(test_y)*timesteps, 1))\n",
    "inv_y = min_max_scaler_y.inverse_transform(test_y)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbe728de-2d64-4330-9584-f4e7682fcb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Export</th>\n",
       "      <th>Import</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>2019_Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>China</td>\n",
       "      <td>12.189871</td>\n",
       "      <td>11.773858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>France</td>\n",
       "      <td>13.117925</td>\n",
       "      <td>12.821405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>13.157817</td>\n",
       "      <td>12.642021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Italy</td>\n",
       "      <td>11.503823</td>\n",
       "      <td>10.430927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>14.166690</td>\n",
       "      <td>14.133159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Norway</td>\n",
       "      <td>12.668027</td>\n",
       "      <td>12.215879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Spain</td>\n",
       "      <td>13.086917</td>\n",
       "      <td>13.113035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>11.157891</td>\n",
       "      <td>9.765879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>10.393670</td>\n",
       "      <td>9.908276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>13.622928</td>\n",
       "      <td>13.206026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>United States</td>\n",
       "      <td>13.271929</td>\n",
       "      <td>12.849195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Norway</td>\n",
       "      <td>China</td>\n",
       "      <td>14.853377</td>\n",
       "      <td>15.274286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Norway</td>\n",
       "      <td>France</td>\n",
       "      <td>15.947841</td>\n",
       "      <td>15.636746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Germany</td>\n",
       "      <td>16.800516</td>\n",
       "      <td>16.527326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>13.026845</td>\n",
       "      <td>13.408566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Italy</td>\n",
       "      <td>14.551989</td>\n",
       "      <td>14.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>16.255209</td>\n",
       "      <td>16.262099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Spain</td>\n",
       "      <td>14.645455</td>\n",
       "      <td>14.673019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>13.141044</td>\n",
       "      <td>12.735290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>13.449379</td>\n",
       "      <td>13.630056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Norway</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>16.992027</td>\n",
       "      <td>16.857625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Norway</td>\n",
       "      <td>United States</td>\n",
       "      <td>15.598198</td>\n",
       "      <td>15.315510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>China</td>\n",
       "      <td>16.918055</td>\n",
       "      <td>16.885190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>France</td>\n",
       "      <td>16.993324</td>\n",
       "      <td>16.779875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Germany</td>\n",
       "      <td>17.706007</td>\n",
       "      <td>17.684628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>11.077947</td>\n",
       "      <td>11.206771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Italy</td>\n",
       "      <td>16.766542</td>\n",
       "      <td>16.598659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>16.039093</td>\n",
       "      <td>15.620471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Norway</td>\n",
       "      <td>13.654879</td>\n",
       "      <td>13.411657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Spain</td>\n",
       "      <td>15.791513</td>\n",
       "      <td>15.879963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>14.695597</td>\n",
       "      <td>15.028037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>16.296560</td>\n",
       "      <td>17.158617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>United States</td>\n",
       "      <td>17.179869</td>\n",
       "      <td>17.596350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>China</td>\n",
       "      <td>15.100914</td>\n",
       "      <td>14.818495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>France</td>\n",
       "      <td>15.810434</td>\n",
       "      <td>15.888942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Germany</td>\n",
       "      <td>16.518320</td>\n",
       "      <td>16.625966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>10.479660</td>\n",
       "      <td>10.062222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Italy</td>\n",
       "      <td>16.082409</td>\n",
       "      <td>16.093127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>15.307070</td>\n",
       "      <td>15.566759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Norway</td>\n",
       "      <td>13.353008</td>\n",
       "      <td>13.499674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Spain</td>\n",
       "      <td>15.644376</td>\n",
       "      <td>15.912189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>14.390831</td>\n",
       "      <td>13.856670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>16.066725</td>\n",
       "      <td>16.238453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>United States</td>\n",
       "      <td>15.840213</td>\n",
       "      <td>16.010333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Export          Import  Prediction  2019_Values\n",
       "0       Iceland           China   12.189871    11.773858\n",
       "1       Iceland          France   13.117925    12.821405\n",
       "2       Iceland         Germany   13.157817    12.642021\n",
       "3       Iceland           Italy   11.503823    10.430927\n",
       "4       Iceland     Netherlands   14.166690    14.133159\n",
       "5       Iceland          Norway   12.668027    12.215879\n",
       "6       Iceland           Spain   13.086917    13.113035\n",
       "7       Iceland     Switzerland   11.157891     9.765879\n",
       "8       Iceland          Turkey   10.393670     9.908276\n",
       "9       Iceland  United Kingdom   13.622928    13.206026\n",
       "10      Iceland   United States   13.271929    12.849195\n",
       "11       Norway           China   14.853377    15.274286\n",
       "12       Norway          France   15.947841    15.636746\n",
       "13       Norway         Germany   16.800516    16.527326\n",
       "14       Norway         Iceland   13.026845    13.408566\n",
       "15       Norway           Italy   14.551989    14.169400\n",
       "16       Norway     Netherlands   16.255209    16.262099\n",
       "17       Norway           Spain   14.645455    14.673019\n",
       "18       Norway     Switzerland   13.141044    12.735290\n",
       "19       Norway          Turkey   13.449379    13.630056\n",
       "20       Norway  United Kingdom   16.992027    16.857625\n",
       "21       Norway   United States   15.598198    15.315510\n",
       "22  Switzerland           China   16.918055    16.885190\n",
       "23  Switzerland          France   16.993324    16.779875\n",
       "24  Switzerland         Germany   17.706007    17.684628\n",
       "25  Switzerland         Iceland   11.077947    11.206771\n",
       "26  Switzerland           Italy   16.766542    16.598659\n",
       "27  Switzerland     Netherlands   16.039093    15.620471\n",
       "28  Switzerland          Norway   13.654879    13.411657\n",
       "29  Switzerland           Spain   15.791513    15.879963\n",
       "30  Switzerland          Turkey   14.695597    15.028037\n",
       "31  Switzerland  United Kingdom   16.296560    17.158617\n",
       "32  Switzerland   United States   17.179869    17.596350\n",
       "33       Turkey           China   15.100914    14.818495\n",
       "34       Turkey          France   15.810434    15.888942\n",
       "35       Turkey         Germany   16.518320    16.625966\n",
       "36       Turkey         Iceland   10.479660    10.062222\n",
       "37       Turkey           Italy   16.082409    16.093127\n",
       "38       Turkey     Netherlands   15.307070    15.566759\n",
       "39       Turkey          Norway   13.353008    13.499674\n",
       "40       Turkey           Spain   15.644376    15.912189\n",
       "41       Turkey     Switzerland   14.390831    13.856670\n",
       "42       Turkey  United Kingdom   16.066725    16.238453\n",
       "43       Turkey   United States   15.840213    16.010333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat = pd.DataFrame(inv_yhat)\n",
    "inv_y = pd.DataFrame(inv_y)\n",
    "inv_yhat_y = pd.concat((inv_yhat, inv_y),axis=1)\n",
    "inv_yhat_y.columns = [\"Prediction\",\"2019_Values\"]\n",
    "countries_df = dff[[\"Export\",\"Import\"]].iloc[:44]\n",
    "prediction_data = pd.concat((countries_df, inv_yhat_y),axis=1)\n",
    "prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7ac1a-82a5-4d00-bdbf-1d77dc8ccad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
